{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparseCovNet Reproducing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed8b0cb8ece9466bb2b9ade91cfaf2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0606551aa79f48d6b7efe5945c060b04",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46b4c566e3744e3b895b6697b9798dec",
              "IPY_MODEL_f6532fa9e9954bfa8a6a6f0e19174d87"
            ]
          }
        },
        "0606551aa79f48d6b7efe5945c060b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46b4c566e3744e3b895b6697b9798dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e27319c117454193abc3046c3585bfc8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2821c3ba9724746a4cf7fe2a8d8fe61"
          }
        },
        "f6532fa9e9954bfa8a6a6f0e19174d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b1b7710bc0a45cbb323b610c0fb426a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 67962028.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aac14895e0244129872c6e072d5f9cd8"
          }
        },
        "e27319c117454193abc3046c3585bfc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2821c3ba9724746a4cf7fe2a8d8fe61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b1b7710bc0a45cbb323b610c0fb426a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aac14895e0244129872c6e072d5f9cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnQsUkTVOnEy"
      },
      "source": [
        "# Github resources related to the original paper:\n",
        "\n",
        "\n",
        "1.   https://github.com/facebookresearch/SparseConvNet\n",
        "2.   https://github.com/btgraham/SparseConvNet-archived\n",
        "3.   https://github.com/btgraham/SparseConvNet-CPU-archived\n",
        "4.   https://github.com/traveller59/spconv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5KCGoZfP_WY"
      },
      "source": [
        "# 1. Reproduce MNIST results on DeepCNet(5,10)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Data preprocessing: shift up to +-2pixels and do data augmentation\n",
        "*   CNN architecture: 6 conv layers, 5 max pooling layers(2,2), ReLU for hidden layers, softmax for output layer, input spatial size: 96,96(by an easy way of padding zero on axis1 and axis2 each side of 34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANj2I1CzQQt4"
      },
      "source": [
        "## 1.1 Download the MNIST dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXolh7YMCe1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81dac856-d65e-4d65-caaf-acbf955614c6"
      },
      "source": [
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-14 02:45:54--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-14 02:45:55--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz.3’\n",
            "\n",
            "MNIST.tar.gz.3          [        <=>         ]  33.20M  6.82MB/s    in 14s     \n",
            "\n",
            "2021-04-14 02:46:09 (2.40 MB/s) - ‘MNIST.tar.gz.3’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yxtgv-iQddH"
      },
      "source": [
        "## 1.2 Prepare the training set and the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYCUOhE4Qnvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6983fdc-dca1-474e-a77c-7d9606c8258c"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.interpolation import shift\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "torch.manual_seed(1)    # reproducible\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2fdbb8c910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-goiniYQQpy3"
      },
      "source": [
        "## 1.3 Define functions and DeepCNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVVuLJ4TKe6h"
      },
      "source": [
        "def try_gpu():\n",
        "    \"\"\"\n",
        "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
        "    as cpu.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQGd_1EsQxze"
      },
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    13-layer DeepCNet: \n",
        "      6 conv layers, \n",
        "      5 max pooling layers(2,2), \n",
        "      ReLU for hidden layers, \n",
        "      Softmax for output layer\n",
        "\n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "        output_resize_para: the parameter used for the final Linear layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, hidden_channels, out_features, output_resize_para):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(         # input shape (1, 96, 96) (should be preprocessed to be sparse)\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,              \n",
        "                out_channels=hidden_channels[0],            \n",
        "                kernel_size=3,              # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=1,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(        \n",
        "            nn.Conv2d(hidden_channels[0], \n",
        "                      hidden_channels[1],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                   \n",
        "            nn.MaxPool2d(2),               \n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[1], \n",
        "                      hidden_channels[2],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[2], \n",
        "                      hidden_channels[3],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        \n",
        "        self.conv5 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[3], \n",
        "                      hidden_channels[4],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "\n",
        "        self.conv6 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[4], \n",
        "                      hidden_channels[5],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "                 \n",
        "        )\n",
        "  \n",
        "        self.fc = nn.Linear(output_resize_para*output_resize_para*hidden_channels[5], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swR6VMY7JnJX"
      },
      "source": [
        "class NetDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    13-layer DeepCNet: \n",
        "      6 conv layers, \n",
        "      5 max pooling layers(2,2), \n",
        "      ReLU for hidden layers, \n",
        "      Softmax for output layer\n",
        "\n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "        output_resize_para: the parameter used for the final Linear layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, hidden_channels, out_features, output_resize_para):\n",
        "        super(NetDropout, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(         # input shape (1, 96, 96) (should be preprocessed to be sparse)\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,              \n",
        "                out_channels=hidden_channels[0],            \n",
        "                kernel_size=3,              # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=1,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(        \n",
        "            nn.Conv2d(hidden_channels[0], \n",
        "                      hidden_channels[1],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                   \n",
        "            nn.MaxPool2d(2),               \n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[1], \n",
        "                      hidden_channels[2],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[2], \n",
        "                      hidden_channels[3],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        \n",
        "        self.conv5 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[3], \n",
        "                      hidden_channels[4],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "\n",
        "        self.conv6 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels[4], \n",
        "                      hidden_channels[5],\n",
        "                      kernel_size=2, \n",
        "                      stride=1,\n",
        "                      padding=1,\n",
        "                      ),     \n",
        "            nn.ReLU(),                     \n",
        "                 \n",
        "        )\n",
        "  \n",
        "        self.fc = nn.Linear(output_resize_para*output_resize_para*hidden_channels[5], out_features)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.dropout4 = nn.Dropout(0.4)\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout1(self.conv3(x))\n",
        "        x = self.dropout2(self.conv4(x))\n",
        "        x = self.dropout3(self.conv5(x))\n",
        "        x = self.dropout4(self.conv6(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout5(self.fc(x))\n",
        "        return x           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGGbhpAWMdz5"
      },
      "source": [
        "def evaluate_accuracy(data_loader, net, device=torch.device('cpu')):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    #init\n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        X = nn.functional.pad(X, (34,34,34,34,0,0,0,0,), 'constant', 0)\n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            outputs = net(X)\n",
        "            acc_sum += torch.sum((torch.argmax(outputs, dim=1) == y))\n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "def train(train_loader, device, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "  # Training loop\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "        # Set to same device\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        x_batch = nn.functional.pad(x_batch, (34,34,34,34,0,0,0,0,), 'constant', 0)\n",
        "        \n",
        "        x_batch = torch.cat([x_batch, torch.roll(x_batch,2,2), torch.roll(x_batch,-2,2)])\n",
        "        y_batch = torch.cat([y_batch, y_batch, y_batch])\n",
        "        \n",
        "        # Set the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform forward pass\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        # Backward computation and update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss)\n",
        "    return train_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOUsqQLdQ4lu"
      },
      "source": [
        "# 1.4 MNIST training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB012OdCKdOK"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "# Download the MNIST dataset\n",
        "train_data = MNIST(root = './', train=True, download=True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "test_data = MNIST(root = './', train=False, download=True, transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "\n",
        "# Data Loader for easy mini-batch\n",
        "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = Data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npve85ORQ724"
      },
      "source": [
        "# training process\n",
        "in_channels = 1 # Black-white images in MNIST digits\n",
        "hidden_channels = [60, 120, 180, 240, 300, 360]\n",
        "out_features = 10 \n",
        "\n",
        "# Training parameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Initialize network\n",
        "net = NetDropout(in_channels, hidden_channels, out_features, 4)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define list to store losses and performances of each iteration\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "train_losses = []\n",
        "\n",
        "\n",
        "# Try using gpu instead of cpu\n",
        "device = try_gpu()\n",
        "train_start = time.time()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Network in training mode and to device\n",
        "    net.to(device)\n",
        "\n",
        "    train_loss = train(train_loader, device, net, criterion, optimizer)\n",
        "    # Compute train and test error\n",
        "    train_acc = 100*evaluate_accuracy(train_loader, net.to('cpu'))\n",
        "    test_acc = 100*evaluate_accuracy(test_loader, net.to('cpu'))\n",
        "    \n",
        "    # Development of performance\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "\n",
        "    # Print performance\n",
        "\n",
        "    print('Epoch: {:.0f}'.format(epoch+1))\n",
        "    print(train_accs)\n",
        "    print(test_accs)\n",
        "    print(train_losses)\n",
        "    # print('Accuracy of train set: {:.2f}%'.format(train_acc))\n",
        "    # print('Accuracy of test set: {:.2f}%'.format(test_acc))\n",
        "    print('')     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzEkkziRFHa"
      },
      "source": [
        "## 1.5 Evaluate and plot the curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEUfXo1ORH5P"
      },
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses, label=\"train\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(train_accs, label = 'train')\n",
        "plt.plot(test_accs, label = 'test')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aubPMs-BTdy1"
      },
      "source": [
        "(Supplementary: Due to the resource constraints, we could not get reasonable results on CIFAR10 and CIFAR100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-lycbyKKS48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "ed8b0cb8ece9466bb2b9ade91cfaf2cc",
            "0606551aa79f48d6b7efe5945c060b04",
            "46b4c566e3744e3b895b6697b9798dec",
            "f6532fa9e9954bfa8a6a6f0e19174d87",
            "e27319c117454193abc3046c3585bfc8",
            "c2821c3ba9724746a4cf7fe2a8d8fe61",
            "0b1b7710bc0a45cbb323b610c0fb426a",
            "aac14895e0244129872c6e072d5f9cd8"
          ]
        },
        "outputId": "0aff6967-648b-4540-a952-058c42882d1f"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=40, translate=(0.2, 0.5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainset_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batch_size,shuffle=True, num_workers=4)\n",
        "\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testset_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed8b0cb8ece9466bb2b9ade91cfaf2cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2AxuAKJMJgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900e1924-7a43-4546-d01d-cf9e2278689c"
      },
      "source": [
        "# training process\n",
        "in_channels = 3 \n",
        "hidden_channels = [300, 600, 900, 1200, 1500, 1800]\n",
        "out_features = 10 \n",
        "\n",
        "# Training parameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Initialize network\n",
        "net = NetDropout(in_channels, hidden_channels, out_features, 4)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define list to store losses and performances of each iteration\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "\n",
        "# Try using gpu instead of cpu\n",
        "device = try_gpu()\n",
        "train_start = time.time()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Network in training mode and to device\n",
        "    net.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    train_loss = train(trainset_loader, device, net, criterion, optimizer)\n",
        "    \n",
        "    # Compute train and test error\n",
        "    train_acc = 100*evaluate_accuracy(trainset_loader, net.to('cpu'))\n",
        "    test_acc = 100*evaluate_accuracy(testset_loader, net.to('cpu'))\n",
        "    \n",
        "    # Development of performance\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Print performance\n",
        "    print('Epoch: {:.0f}'.format(epoch+1))\n",
        "    print('Accuracy of train set: {:.2f}%'.format(train_acc))\n",
        "    print('Accuracy of test set: {:.2f}%'.format(test_acc))\n",
        "    print('Epoch training time: {:.2f}'.format(time.time()-train_start))\n",
        "    print('')    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Accuracy of train set: 22.55%\n",
            "Accuracy of test set: 23.93%\n",
            "Epoch training time: 8950.56\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y27UPyUQrvM7"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS8rohMPuM0p"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00208/Online%20Handwritten%20Assamese%20Characters%20Dataset.rar\n",
        "!unrar e -idq -cl -y \"Online Handwritten Assamese Characters Dataset.rar\"\n",
        "!mkdir input\n",
        "!mv *.txt input/\n",
        "!grep \"CHARACTER_NAME\" input/*.txt > labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAtaWfCqZZz"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_features,output_resize = 3):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, hidden_channels[0],kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(hidden_channels[0], hidden_channels[1],kernel_size=5,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(hidden_channels[1], hidden_channels[2],kernel_size=5,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fn = nn.Linear(output_resize*output_resize*hidden_channels[2], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_accuracy(data_loader, net, device=torch.device('cpu')):\n",
        "    net.eval()  \n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
        "            n += y.shape[0]\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi8Wr5BNsZpz"
      },
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import glob\n",
        "\n",
        "class AssameseData(Dataset):\n",
        "\n",
        "    def _read_img(self, path, scaling=173):\n",
        "        with open(path, 'r') as f:\n",
        "            # print(\"Processing: \" + path)\n",
        "            label = f.readline()\n",
        "            label = label.split()[1].strip()\n",
        "            stoke = f.readline()\n",
        "            stoke = stoke.split()[1].strip()\n",
        "            f.readline()\n",
        "            img = np.zeros((4868 // scaling, 4868 // scaling))\n",
        "            for line in f.readlines():\n",
        "                if (\"PEN_DOWN\" not in line and \"PEN_UP\" not in line and \"END_CHARACTER\" not in line):\n",
        "                    x, y = tuple(map(int, map(str.strip, line.split()[0:2])))\n",
        "                    img[x // scaling, y // scaling] = 254\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __init__(self, input_dir, transform=None):\n",
        "        self.input_dir = input_dir\n",
        "        self.transform = transform\n",
        "        self.texts = glob.glob(self.input_dir + \"/*.txt\")\n",
        "        self.length = len(self.texts)\n",
        "\n",
        "        y = []\n",
        "        with open('labels', 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                y.append(line.strip().split()[-1].strip())\n",
        "\n",
        "        p = set(y)\n",
        "        print(p)\n",
        "\n",
        "        self.label_mapping = {el: num for num, el in enumerate(p)}\n",
        "        print(self.label_mapping)\n",
        "        print(len(self.label_mapping))\n",
        "        print((max(self.label_mapping.values())))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self._read_img(self.texts[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # image = torch.tensor(np.reshape(image, (1, *image.shape)), dtype=torch.float)\n",
        "        image = image.float()\n",
        "\n",
        "        label = torch.tensor(self.label_mapping[label])\n",
        "\n",
        "\n",
        "\n",
        "        sample = (image, label)\n",
        "\n",
        "        return sample\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLU_HhrhsmN7"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1201,), (0.3071,))])\n",
        "\n",
        "\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "# data = AssameseData('input', transform)\n",
        "#\n",
        "# split = int(len(data) * 0.8)\n",
        "#\n",
        "# train, test = random_split(data, [split, len(data) - split])\n",
        "# train_loader = DataLoader(data, batch_size=16, shuffle=True)\n",
        "# test_loader = DataLoader(train, batch_size=16)\n",
        "#\n",
        "# train_data, test_data = train, test\n",
        "\n",
        "# split = int(len(data) * 0.8)\n",
        "#\n",
        "# train, test = random_split(data, [split, len(data) - split])\n",
        "# train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
        "# test_loader = DataLoader(train, batch_size=64)\n",
        "\n",
        "\n",
        "in_channels = 1\n",
        "hidden_channels = [4, 5, 6]\n",
        "out_features = 10 #len(data.label_mapping)\n",
        "learning_rate = 0.001\n",
        "epochs = 80\n",
        "\n",
        "net = Net(in_channels, hidden_channels, out_features)\n",
        "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)#torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "device = try_gpu()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    net.train()\n",
        "    net.to(device)\n",
        "\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = net(x_batch.float())\n",
        "\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_acc = 100 * evaluate_accuracy(train_loader, net.to('cpu'))\n",
        "    test_acc = 100 * evaluate_accuracy(test_loader, net.to('cpu'))\n",
        "\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print('Epoch: {:.0f}'.format(epoch + 1))\n",
        "    print('Accuracy of train set: '+str(train_acc))\n",
        "    print('Accuracy of test set: '+str(test_acc))\n",
        "    print('Training Time Of Epoch: ' + str(end - start))\n",
        "    print('')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}